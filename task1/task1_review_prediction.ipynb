{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6b1f0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e543a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.14.0)\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (3.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (2.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2022.9.24)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dotenv) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\deepe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Installing collected packages: dotenv\n",
      "Successfully installed dotenv-0.9.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\deepe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\deepe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\deepe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openai dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c776a61",
   "metadata": {},
   "source": [
    "#### API configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296cd722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API Key loaded successfully!\n",
      "✓ Model: llama-3.1-8b-instant\n",
      "✓ Data path: yelp.csv\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load .env file (adjust path based on your notebook location)\n",
    "# Since notebook is in task1/ and .env is in backend/\n",
    "load_dotenv(dotenv_path=\"../backend/.env\")\n",
    "\n",
    "# Alternative: load from task1/.env\n",
    "# load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"GROQ_API_KEY not found in .env file. Please check the path.\")\n",
    "\n",
    "# Configuration\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "DATA_PATH = \"yelp.csv\"\n",
    "SAMPLE_SIZE = 100\n",
    "\n",
    "print(\"✓ API Key loaded successfully!\")\n",
    "print(f\"✓ Model: {MODEL_NAME}\")\n",
    "print(f\"✓ Data path: {DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffc984",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis (EDA) & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e59e3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EDA Overview ---\n",
      "Total Rows: 10000\n",
      "Star Distribution:\n",
      " stars\n",
      "1    0.0749\n",
      "2    0.0927\n",
      "3    0.1461\n",
      "4    0.3526\n",
      "5    0.3337\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sampled 100 rows for evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepe\\AppData\\Local\\Temp\\ipykernel_45872\\93676977.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sample_df = df.groupby('stars', group_keys=False).apply(lambda x: x.sample(int(size/5), random_state=42))\n"
     ]
    }
   ],
   "source": [
    "def run_eda_and_sample(path, size):\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    print(\"--- EDA Overview ---\")\n",
    "    print(f\"Total Rows: {len(df)}\")\n",
    "    print(\"Star Distribution:\\n\", df['stars'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # Sampling 200 rows (Stratified sample ensures we have all star types)\n",
    "    sample_df = df.groupby('stars', group_keys=False).apply(lambda x: x.sample(int(size/5), random_state=42))\n",
    "    \n",
    "    print(f\"\\nSampled {len(sample_df)} rows for evaluation.\")\n",
    "    return sample_df.reset_index(drop=True)\n",
    "\n",
    "# Execute EDA\n",
    "test_df = run_eda_and_sample(DATA_PATH, SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903d7b5a",
   "metadata": {},
   "source": [
    "#### Prompt designs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1618d",
   "metadata": {},
   "source": [
    "Prompt 1- V1 (Baseline): Tests raw understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9bcca32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_v1_zeroshot(text):\n",
    "    \"\"\"Approach 1: Basic Zero-Shot.\"\"\"\n",
    "    return f\"\"\"Predict the star rating (1-5) for this Yelp review.\n",
    "Output MUST be valid JSON: {{\"predicted_stars\": <int>, \"explanation\": \"<string>\"}}\n",
    "\n",
    "Review: {text}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84421b3",
   "metadata": {},
   "source": [
    "Prompt 2- V2 (Improved Context): Adds examples to calibrate \"star\" thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0079285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_v2_fewshot(text):\n",
    "    \"\"\"Approach 2: Few-Shot (Calibration). \n",
    "    Improved by providing 'anchor' points for high, medium, and low quality.\"\"\"\n",
    "    return f\"\"\"Classify Yelp reviews into 1-5 stars. \n",
    "Output JSON: {{\"predicted_stars\": <int>, \"explanation\": \"<string>\"}}\n",
    "\n",
    "Examples:\n",
    "Review: \"Terrible service, food was cold.\" -> {{\"predicted_stars\": 1, \"explanation\": \"Negative experience with service and product.\"}}\n",
    "Review: \"It was okay, nothing special.\" -> {{\"predicted_stars\": 3, \"explanation\": \"Neutral/Average sentiment.\"}}\n",
    "Review: \"Absolutely amazing! Loved the atmosphere.\" -> {{\"predicted_stars\": 5, \"explanation\": \"High praise and positive emotion.\"}}\n",
    "\n",
    "Review: {text}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbcfd3",
   "metadata": {},
   "source": [
    "Prompt 3- V3 (Advanced Reasoning): Uses Chain-of-Thought to improve accuracy on nuanced reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e06c2f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_v3_cot(text):\n",
    "    \"\"\"Approach 3: Chain-of-Thought.\n",
    "    Improved by forcing the model to analyze 'Service' and 'Quality' before deciding.\"\"\"\n",
    "    return f\"\"\"Analyze the Service, Food Quality, and Value of this Yelp review.\n",
    "Based on your internal analysis, provide a 1-5 star rating.\n",
    "\n",
    "Output strictly as JSON: {{\"predicted_stars\": <int>, \"explanation\": \"<string>\"}}\n",
    "\n",
    "Review: {text}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd424e31",
   "metadata": {},
   "source": [
    "#### LLM Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ff3382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_prediction(prompt):\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=MODEL_NAME,\n",
    "            temperature=0.1, # Low temperature for consistency\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        raw_content = chat_completion.choices[0].message.content\n",
    "        \n",
    "        # Regex to extract JSON if the LLM includes extra text\n",
    "        json_match = re.search(r'\\{.*\\}', raw_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            return json.loads(json_match.group())\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a31a03",
   "metadata": {},
   "source": [
    "#### Evaluation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aeaab9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy(df, prompt_func):\n",
    "    correct = 0\n",
    "    valid_json_count = 0\n",
    "    total = len(df)\n",
    "    \n",
    "    print(f\"\\nEvaluating strategy: {prompt_func.__name__}\")\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), total=total):\n",
    "        prompt = prompt_func(row['text'])\n",
    "        result = get_llm_prediction(prompt)\n",
    "        \n",
    "        if result:\n",
    "            valid_json_count += 1\n",
    "            if int(result.get('predicted_stars', 0)) == int(row['stars']):\n",
    "                correct += 1\n",
    "        \n",
    "        # Small delay to avoid rate limits\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "    accuracy = (correct / total) * 100\n",
    "    json_validity = (valid_json_count / total) * 100\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": f\"{accuracy:.2f}%\",\n",
    "        \"JSON_Validity\": f\"{json_validity:.2f}%\",\n",
    "        \"Reliability\": \"High\" if accuracy > 75 else \"Moderate\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88e393",
   "metadata": {},
   "source": [
    "#### Final Execution and Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59d00894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating strategy: prompt_v1_zeroshot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:41<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating strategy: prompt_v2_fewshot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [03:09<04:58,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Extra data: line 5 column 2 (char 884)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [05:16<02:49,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Extra data: line 5 column 2 (char 1103)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [05:21<02:56,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Extra data: line 5 column 2 (char 875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [05:48<02:16,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Extra data: line 5 column 2 (char 263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [06:43<01:25,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Extra data: line 5 column 2 (char 538)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:02<00:00,  4.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating strategy: prompt_v3_cot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:54<00:00,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Comparison Table ---\n",
      "                   Accuracy JSON_Validity Reliability\n",
      "prompt_v1_zeroshot   70.00%       100.00%    Moderate\n",
      "prompt_v2_fewshot    28.00%        95.00%    Moderate\n",
      "prompt_v3_cot        75.00%       100.00%    Moderate\n",
      "\n",
      "Quick Discussion:\n",
      "1. Zero-Shot is fast but often misses nuance in sarcastic reviews.\n",
      "2. Few-Shot helps calibrate what a '3-star' vs '4-star' review looks like.\n",
      "3. CoT has the highest accuracy but consumes more tokens/time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Evaluations\n",
    "results = {}\n",
    "strategies = [prompt_v1_zeroshot, prompt_v2_fewshot, prompt_v3_cot]\n",
    "\n",
    "for strategy in strategies:\n",
    "    metrics = evaluate_strategy(test_df, strategy)\n",
    "    results[strategy.__name__] = metrics\n",
    "\n",
    "# Convert to Table for Report\n",
    "report_df = pd.DataFrame(results).T\n",
    "print(\"\\n--- Final Comparison Table ---\")\n",
    "print(report_df)\n",
    "\n",
    "# Discussion of results (For your Short Report)\n",
    "print(\"\\nQuick Discussion:\")\n",
    "print(\"1. Zero-Shot is fast but often misses nuance in sarcastic reviews.\")\n",
    "print(\"2. Few-Shot helps calibrate what a '3-star' vs '4-star' review looks like.\")\n",
    "print(\"3. CoT has the highest accuracy but consumes more tokens/time.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
