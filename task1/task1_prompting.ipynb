{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f0058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296cd722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31111209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yelp_data(csv_path: str, sample_size: int = 200, seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads Yelp reviews dataset and samples a subset for evaluation.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    required_columns = {\"text\", \"stars\"}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(\"Dataset must contain 'text' and 'stars' columns\")\n",
    "    \n",
    "    df = df.sample(n=sample_size, random_state=seed).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696fd0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    r\"C:\\Users\\deepe\\Downloads\\fynd_ai_intern_assignment\\yelp.csv\",\n",
    "    nrows=200\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721fe269",
   "metadata": {},
   "source": [
    "LLM Client Wrapper (Gemini / OpenRouter / Any LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "371c32e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to the LLM and returns raw text response.\n",
    "    Replace this with actual API call.\n",
    "    \"\"\"\n",
    "    # --- MOCK EXAMPLE ---\n",
    "    # In real implementation, call Gemini / OpenRouter here\n",
    "    return \"\"\"\n",
    "    {\n",
    "      \"predicted_stars\": 4,\n",
    "      \"explanation\": \"The review expresses overall satisfaction with minor complaints.\"\n",
    "    }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897e9b5f",
   "metadata": {},
   "source": [
    "Prompt V1 — Simple Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58da34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_v1(review_text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a sentiment analysis assistant.\n",
    "\n",
    "Classify the following Yelp review into a star rating from 1 to 5.\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\n",
    "Return JSON in the following format:\n",
    "{{\n",
    "  \"predicted_stars\": number,\n",
    "  \"explanation\": \"short reasoning\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80611f",
   "metadata": {},
   "source": [
    "Prompt V2 — Rubric-Based Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d6d7cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_v2(review_text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are an expert reviewer classifier.\n",
    "\n",
    "Use the following rubric:\n",
    "1 star: extremely negative experience\n",
    "2 stars: mostly negative\n",
    "3 stars: mixed or neutral\n",
    "4 stars: mostly positive\n",
    "5 stars: extremely positive and enthusiastic\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"predicted_stars\": number,\n",
    "  \"explanation\": \"short reasoning\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a527ebf",
   "metadata": {},
   "source": [
    "Prompt V3 — Structured Reasoning (Hidden Chain-of-Thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ddac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_v3(review_text: str) -> str:\n",
    "    return f\"\"\"\n",
    "You are a professional review analyst.\n",
    "\n",
    "Internally:\n",
    "1. Determine sentiment polarity\n",
    "2. Determine sentiment strength\n",
    "3. Map sentiment to a star rating\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\n",
    "Return ONLY the final JSON:\n",
    "{{\n",
    "  \"predicted_stars\": number,\n",
    "  \"explanation\": \"concise justification\"\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893da8dd",
   "metadata": {},
   "source": [
    "JSON Parsing & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82cfbb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llm_response(response_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Safely parses LLM JSON response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(response_text.strip())\n",
    "        if \"predicted_stars\" not in data:\n",
    "            raise ValueError(\"Missing predicted_stars\")\n",
    "        return data\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057e3f7",
   "metadata": {},
   "source": [
    "Single Prompt Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104f3c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(\n",
    "    df: pd.DataFrame,\n",
    "    prompt_fn,\n",
    "    max_retries: int = 2\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs evaluation for a given prompt function.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        review = row[\"text\"]\n",
    "        actual = int(row[\"stars\"])\n",
    "\n",
    "        parsed = None\n",
    "        for _ in range(max_retries):\n",
    "            response = call_llm(prompt_fn(review))\n",
    "            parsed = parse_llm_response(response)\n",
    "            if parsed:\n",
    "                break\n",
    "\n",
    "        if parsed:\n",
    "            predicted = int(parsed[\"predicted_stars\"])\n",
    "            valid_json = True\n",
    "        else:\n",
    "            predicted = None\n",
    "            valid_json = False\n",
    "\n",
    "        results.append({\n",
    "            \"actual_stars\": actual,\n",
    "            \"predicted_stars\": predicted,\n",
    "            \"json_valid\": valid_json\n",
    "        })\n",
    "\n",
    "        time.sleep(0.2)  # rate limiting\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95939f01",
   "metadata": {},
   "source": [
    "Metric Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcf7bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(results_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Computes accuracy and JSON validity.\n",
    "    \"\"\"\n",
    "    valid_df = results_df[results_df[\"json_valid\"] == True]\n",
    "\n",
    "    accuracy = (\n",
    "        (valid_df[\"actual_stars\"] == valid_df[\"predicted_stars\"]).mean()\n",
    "        if len(valid_df) > 0 else 0\n",
    "    )\n",
    "\n",
    "    json_valid_rate = results_df[\"json_valid\"].mean()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": round(accuracy * 100, 2),\n",
    "        \"json_valid_rate\": round(json_valid_rate * 100, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae88428b",
   "metadata": {},
   "source": [
    "Run Evaluation for all the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124f0c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:40<00:00,  4.93it/s]\n",
      "100%|██████████| 200/200 [00:40<00:00,  4.93it/s]\n",
      "100%|██████████| 200/200 [00:40<00:00,  4.94it/s]\n"
     ]
    }
   ],
   "source": [
    "results_v1 = evaluate_prompt(df, prompt_v1)\n",
    "results_v2 = evaluate_prompt(df, prompt_v2)\n",
    "results_v3 = evaluate_prompt(df, prompt_v3)\n",
    "\n",
    "metrics_v1 = compute_metrics(results_v1)\n",
    "metrics_v2 = compute_metrics(results_v2)\n",
    "metrics_v3 = compute_metrics(results_v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981f052",
   "metadata": {},
   "source": [
    "Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e30bb016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>json_valid_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V1 - Zero Shot</td>\n",
       "      <td>35.5</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V2 - Rubric Based</td>\n",
       "      <td>35.5</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V3 - Structured Reasoning</td>\n",
       "      <td>35.5</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Prompt  accuracy  json_valid_rate\n",
       "0             V1 - Zero Shot      35.5            100.0\n",
       "1          V2 - Rubric Based      35.5            100.0\n",
       "2  V3 - Structured Reasoning      35.5            100.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame([\n",
    "    {\"Prompt\": \"V1 - Zero Shot\", **metrics_v1},\n",
    "    {\"Prompt\": \"V2 - Rubric Based\", **metrics_v2},\n",
    "    {\"Prompt\": \"V3 - Structured Reasoning\", **metrics_v3},\n",
    "])\n",
    "\n",
    "comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
